{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditCardFraudDetection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"creditcard.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ac768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in ['Class']]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_unscaled\")\n",
    "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "\n",
    "final_df = df_transformed.select(\"features\", col(\"Class\").alias(\"label\"))\n",
    "pandas_df = final_df.toPandas()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
